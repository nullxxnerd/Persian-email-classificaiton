{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"persian_emails.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>موضوع: درخواست پشتیبانی برای بیش از حد داغ می‌...</td>\n",
       "      <td>customer_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>موضوع: شکایت از اسپیکر بلوتوثی قابل حمل\\n\\nبا ...</td>\n",
       "      <td>customer_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>موضوع: شکایت از اسپیکر بلوتوثی قابل حمل\\n\\nدرو...</td>\n",
       "      <td>customer_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>موضوع: شرایط خرید سرویس اینترنت\\n\\nدرود بی‌کرا...</td>\n",
       "      <td>sales_inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>موضوع: درخواست پشتیبانی برای اتصال بلوتوث قطع ...</td>\n",
       "      <td>customer_support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             label\n",
       "0  موضوع: درخواست پشتیبانی برای بیش از حد داغ می‌...  customer_support\n",
       "1  موضوع: شکایت از اسپیکر بلوتوثی قابل حمل\\n\\nبا ...  customer_support\n",
       "2  موضوع: شکایت از اسپیکر بلوتوثی قابل حمل\\n\\nدرو...  customer_support\n",
       "3  موضوع: شرایط خرید سرویس اینترنت\\n\\nدرود بی‌کرا...     sales_inquiry\n",
       "4  موضوع: درخواست پشتیبانی برای اتصال بلوتوث قطع ...  customer_support"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done. DataLoaders are ready—no label leakage.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 0) Define your Persian stop‑words\n",
    "stop_words = {\n",
    "    'و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای', 'آن', 'یک', 'خود',\n",
    "    'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'تواند', 'باشد', 'شد', 'اما', 'دارد',\n",
    "    'باید', 'او', 'می', 'دهد', 'یا', 'همه', 'کنند', 'اگر', 'آنها', 'بود', 'وی',\n",
    "    'کنید', 'کند', 'داده', 'بوده', 'دارند', 'شود', 'چون', 'جز', 'من', 'ما',\n",
    "    'تو', 'شما', 'ایشان'\n",
    "}\n",
    "\n",
    "# 1) Drop rows with null text/label\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "# 2) Map labels → integers\n",
    "label_map = {\n",
    "    'customer_support': 0,\n",
    "    'sales_inquiry':    1,\n",
    "    'partnership':      2,\n",
    "    'spam':             3\n",
    "}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# 3) Clean & tokenize function\n",
    "def preprocess_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text.lower())\n",
    "    text = re.sub(r'[\\d%s]+' % re.escape(string.punctuation), ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 4) Split features and target, then stratify\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 5) Fit tokenizer on train only\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 6) Convert to sequences and determine max length\n",
    "train_seqs = tokenizer.texts_to_sequences(X_train)\n",
    "max_len = min(200, int(np.percentile([len(s) for s in train_seqs], 95)))\n",
    "train_padded = pad_sequences(train_seqs, maxlen=max_len, padding='post')\n",
    "\n",
    "test_seqs = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_seqs, maxlen=max_len, padding='post')\n",
    "\n",
    "# 7) Build DataLoaders\n",
    "def to_loader(X_arr, y_arr, bs=32, shuffle=False):\n",
    "    Xt = torch.tensor(X_arr, dtype=torch.long)\n",
    "    yt = torch.tensor(y_arr.values, dtype=torch.long)\n",
    "    ds = TensorDataset(Xt, yt)\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "train_loader = to_loader(train_padded, y_train, bs=32, shuffle=True)\n",
    "test_loader  = to_loader(test_padded,  y_test,  bs=32)\n",
    "\n",
    "print(\"Preprocessing done. DataLoaders are ready—no label leakage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 36)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 50/50 [00:00<00:00, 152.04it/s]\n",
      "Epoch 1/10, Loss: 0.6428\n",
      "Epoch 1/10, Loss: 0.6428\n",
      "Epoch 1/10, Loss: 0.6428\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 2/10: 100%|██████████| 50/50 [00:00<00:00, 170.12it/s]\n",
      "Epoch 2/10, Loss: 0.0622\n",
      "Epoch 2/10, Loss: 0.0622\n",
      "Epoch 2/10, Loss: 0.0622\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 3/10: 100%|██████████| 50/50 [00:00<00:00, 180.31it/s]\n",
      "Epoch 3/10, Loss: 0.0250\n",
      "Epoch 3/10, Loss: 0.0250\n",
      "Epoch 3/10, Loss: 0.0250\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 4/10: 100%|██████████| 50/50 [00:00<00:00, 168.94it/s]\n",
      "Epoch 4/10, Loss: 0.0157\n",
      "Epoch 4/10, Loss: 0.0157\n",
      "Epoch 4/10, Loss: 0.0157\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 5/10: 100%|██████████| 50/50 [00:00<00:00, 171.02it/s]\n",
      "Epoch 5/10, Loss: 0.0102\n",
      "Epoch 5/10, Loss: 0.0102\n",
      "Epoch 5/10, Loss: 0.0102\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 6/10: 100%|██████████| 50/50 [00:00<00:00, 170.35it/s]\n",
      "Epoch 6/10, Loss: 0.0068\n",
      "Epoch 6/10, Loss: 0.0068\n",
      "Epoch 6/10, Loss: 0.0068\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 7/10: 100%|██████████| 50/50 [00:00<00:00, 178.78it/s]\n",
      "Epoch 7/10, Loss: 0.0043\n",
      "Epoch 7/10, Loss: 0.0043\n",
      "Epoch 7/10, Loss: 0.0043\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 8/10: 100%|██████████| 50/50 [00:00<00:00, 178.98it/s]\n",
      "Epoch 8/10, Loss: 0.0041\n",
      "Epoch 8/10, Loss: 0.0041\n",
      "Epoch 8/10, Loss: 0.0041\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 9/10: 100%|██████████| 50/50 [00:00<00:00, 177.27it/s]\n",
      "Epoch 9/10, Loss: 0.0036\n",
      "Epoch 9/10, Loss: 0.0036\n",
      "Epoch 9/10, Loss: 0.0036\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Epoch 10/10: 100%|██████████| 50/50 [00:00<00:00, 179.81it/s]\n",
      "Epoch 10/10, Loss: 0.0022\n",
      "Epoch 10/10, Loss: 0.0022\n",
      "Epoch 10/10, Loss: 0.0022\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the CNN model\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        x = x.transpose(1, 2)  # (batch_size, embedding_dim, seq_len)\n",
    "        x = [F.relu(conv(x)) for conv in self.convs]  # [(batch_size, num_filters, seq_len - fs + 1), ...]\n",
    "        x = [F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2) for conv_out in x]  # [(batch_size, num_filters), ...]\n",
    "        x = torch.cat(x, 1)  # (batch_size, num_filters * len(filter_sizes))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, num_epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        logger.info(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 5001  # Adjust based on tokenizer\n",
    "embedding_dim = 100\n",
    "num_filters = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_classes = 4  # Adjust based on your dataset\n",
    "dropout = 0.6\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize and train the model\n",
    "model = TextCNN(vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout).to(device)\n",
    "trained_model = train_model(model, train_loader, test_loader, num_epochs, learning_rate, device)\n",
    "\n",
    "# Evaluate the final model on test set\n",
    "trained_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = trained_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "cnn_accuracy = correct / total\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy: 1.0000\n",
      "SVM Test Accuracy: 1.0000\n",
      "\n",
      "Accuracy Comparison:\n",
      "CNN: 1.0000\n",
      "Logistic Regression: 1.0000\n",
      "SVM: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # To match the CNN's vocab size\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "print(f\"Logistic Regression Test Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# Train SVM\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "print(f\"SVM Test Accuracy: {svm_accuracy:.4f}\")\n",
    "\n",
    "# Compare accuracies (assuming cnn_accuracy is available from the previous code block)\n",
    "print(\"\\nAccuracy Comparison:\")\n",
    "print(f\"CNN: {cnn_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression: {lr_accuracy:.4f}\")\n",
    "print(f\"SVM: {svm_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partnership tensor([0.0057, 0.1671, 0.8208, 0.0064])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Invert your label_map for human‑readable output\n",
    "label_map_inv = {0: 'customer_support',\n",
    "                 1: 'sales_inquiry',\n",
    "                 2: 'partnership',\n",
    "                 3: 'spam'}\n",
    "\n",
    "def predict_text(text: str,\n",
    "                 model: torch.nn.Module,\n",
    "                 tokenizer,\n",
    "                 max_len: int,\n",
    "                 device: torch.device) -> (str, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Preprocesses a raw Persian string, tokenizes, pads, runs through the trained TextCNN,\n",
    "    and returns (predicted_label, softmax_probs).\n",
    "    \"\"\"\n",
    "    # 1) Clean & tokenize (must match your training pipeline)\n",
    "    text = unicodedata.normalize('NFKC', text.lower())\n",
    "    text = re.sub(r'[\\d%s]+' % re.escape(string.punctuation), ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    cleaned = ' '.join(tokens)\n",
    "\n",
    "    # 2) Sequence & pad\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    # 3) To tensor\n",
    "    inp = torch.tensor(padded, dtype=torch.long).to(device)\n",
    "\n",
    "    # 4) Forward\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inp)                   # (1, num_classes)\n",
    "        probs  = F.softmax(logits, dim=1).squeeze(0)  # (num_classes,)\n",
    "\n",
    "    # 5) Decode\n",
    "    idx = torch.argmax(probs).item()\n",
    "    label = label_map_inv[idx]\n",
    "    return label, probs.cpu()\n",
    "\n",
    "# Example usage:\n",
    "label, probs = predict_text(\"سلام درخواست همکاری در حوزه دارم با سپاش\", trained_model, tokenizer, max_len, device)\n",
    "print(label, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete: textcnn.pth, tokenizer.pkl, config.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# This script saves the trained TextCNN model and associated tokenizer and config.\n",
    "# Run this after training completes and the following variables are available in your namespace:\n",
    "# `trained_model`, `tokenizer`, `max_len`, `label_map_inv`, `stop_words`.\n",
    "\n",
    "# Save model state_dict\n",
    "torch.save(trained_model.state_dict(), 'textcnn.pth')\n",
    "\n",
    "# Save tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save configuration (preprocessing & model architecture)\n",
    "model_config = {\n",
    "    'vocab_size': trained_model.embedding.num_embeddings,\n",
    "    'embedding_dim': trained_model.embedding.embedding_dim,\n",
    "    'num_filters': trained_model.convs[0].out_channels,\n",
    "    'filter_sizes': [conv.kernel_size[0] for conv in trained_model.convs],\n",
    "    'num_classes': trained_model.fc.out_features,\n",
    "    'dropout': trained_model.dropout.p\n",
    "}\n",
    "config = {\n",
    "    'max_len': max_len,\n",
    "    'label_map_inv': label_map_inv,\n",
    "    'stop_words': stop_words,\n",
    "    'model_config': model_config\n",
    "}\n",
    "with open('config.pkl', 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print('Export complete: textcnn.pth, tokenizer.pkl, config.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
